                      Probability


->  Random experiment :- 
              •  If in each trial of an experiment conducted under identical conditions, the outcome is not unique but 
                 may be any one of the possible outcomes.
              •  eg. (1) Tossing a coin
                     (2) Rolling a dice
                     (3) Drawing 2 red balls from a bag of 3 different color balls


->  Outcomes :-
              •  Result of random experiment 


->  Exhaustive events :-
              •  Total number of possible outcomes of a randomn experiment


->  Mutually exclusive :-
              •  In two events if happening of any one of them exclude the happenig of all the others.
              •  eg. While tossing a coin, heads or tails cannot come at the same time therefore they are mutually 
                     exclusive.


->  Independent events :-
              •  In 2 events, if happening of one is not affected by the happening of the others.


->  eg. In a bag there are 3 white and 2 red balls, 2 balls are drawn
        Outcomes : 2W        <-|
                   2R        <-|-> Exclusive or disjoint events (or case)
                   1W  1R    <-|
                    ↑  ↑
              Independent events (and case)


->  Equally likely :- If the chances/probability of 2 events to occcur is same


->  Probabilty :
              |-------------------------------------|
              |  P(A) = No. of Favourable outcomes  |
              |        -----------------------------|
              |       T-No. of exxhaustive outcomes |
              |-------------------------------------|


->  Sample space : Set of all possible outcomes of a random experiment (denoted by S)


->  Axiomic Probability :
              •  P(A) >= 0               (Axiom of non-negativity)
              •  P(S) = 1                (Axiom of certainity)
              •  P(∏ Ai) = ∑P(Ai)        (Axioms of additivity)
                  (From i = 1 to n/∞)


->  Addition theorem of probability :
              |------------------------------------|
              | P(A ∪ B) = P(A) + P(B) - P(A ∩ B); |
              |------------------------------------|

              |-----------------------|
              | P(A ∪ B) = n(A ∪ B)   | 
              |            --------   |
              |              n(S)     |
              |-----------------------|

              |------------------|
              | P(A') = 1 - P(A) |
              |------------------| 


->  Conditional probability :  We need this probability concept when it is known that event B has occured already, and
                               to find the probability of event A under the condition of event B. (denoted by P(A|B))

              |-------------------|
              | P(A|B) = P(A ∩ B) |
              |          -------- |          where P(B) ≠ 0
              |            P(B)   |
              |-------------------|


->  Multiplication rule :
          Generalisation =>
              |-----------------------------------------------------------|
              | P(A1 ∩ A2 ∩ A3...) = (P(A1))*(P(A2|A1))*(P(A3|A1 ∩ A2)... |
              |-----------------------------------------------------------|          


->  If A & B are independent events so that each occurence or non- occurence of A is not affected by occurence or 
    non - occurence of B,then :
              |-------------------------------|
              | P(A|B) = P(A) or P(B|A) = P(B)|
              |-------------------------------| 


->  Multipllication Theorem of independent events :
							|------------------------|
							| P(A ∩ B) = P(A) * P(B) |
							|------------------------|


->  Total no. of conditions for mutual independence of A1,A2,...,An is :
              = nc2 + nc3 + ... + ncn
              = (nc0 + nc1 + nc3 + ... + ncn) - nc0 - nc1
              = 2^n - 1 - n


->  Mutually exclusive (disjoint) events with positive probabilities have intersection as ø i.e. probabity of intersection
    of events is 0.
	

->  ** If two events are Disjoint then they can't be independent and vice-versa.


->  Partition : Let A = {A1,A2,...,An} be a finite collection of events. Then A is a partition of sample space S if following
    conditions hold :-
              •  P(Ai) >= 0      (for each i)
              •  Events Ai are pairwise disjoints i.e., Ai ∩ Aj = ø   (for i ≠ j)
              •  Union of the events Ai equals to the sample space s i.e. Ui Ai = S 


->  Total probability Theorem :
              Let A = {A1,A2,...,An} be a partition of the sample space S. If B is any event, then -:
                              |------------------------|
                              | P(B) = ∑P(Ai)* P(B|Ai) |
                              |------------------------|


->  Bayes' Theorem :-
    If A1,A2,...,An are mutually disjoint events with P(Ai) ≠ 0 , then for any arbitrary event E which is subset of Ai such 
    that P(E) > 0 , we have 
                              |----------------------------|
                              | P(Ai|E) = P(Ai) * P(E|Ai)  |
                              |          ----------------- |
                              |          ∑(P(Ai) * P(E|Ai))|
                              |----------------------------|


->  Random Variable : A real number X associated with the outcomes of a random experiment.


->  Types of random variable :- 
                •  Discrete random variable => finite or countable number of outcomes in a sample space
                •  Continuous random variable => uncountable number of outcomes in a sample space


->  P{roperties of distribution function : 
                (1) |-----------------------------|
                    |  P(a < X ≤ b) = F(b) - F(a) |
                    |-----------------------------|
    As P(x = a) is always 0 therefore,
    P(a < X ≤ b) = P(a < X < b) = P(a ≤ X ≤ b) = P(a ≤ X < b)

                (2) |---------------|
                    |  0 ≤ F(x) ≤ 1 |
                    |---------------|

                (3) F is non-decreasing
                    |---------------------|
                    |  F(x) ≤ F(y) ,x < y |
                    |---------------------|

                (4) F is right continuous , for all x E R
                    |-----------------------------|
                    |  F(x+0) = lim F(x+h) = F(x) |
                    |           h->0              |
                    |-----------------------------|

                (5) |-------------------------|
                    | F(-∞) = 0   &  F(∞) = 1 |
                    |-------------------------|


->  Joint Density fn. :-
                (for discrete) >>
                    |----------------------|
                    | (¡) p(x,y) ≥ 0       |
                    | (¡¡) ∑x∑y p(x,y) = 1 | 
                    |----------------------|

                (for continuous) >>
                    |--------------------------|
                    | (¡) p(x,y) ≥ 0           |
                    | (¡¡) ∫x∫y p(x,y)dydx = 1 |
                    |--------------------------|


->  Marginal density fn.:
                (for discrete) >>
                    |--------------------------|
                    | p(x) = ∑y P(X = x,Y = y) |
                    |--------------------------|
                    | p(y) = ∑x P(X = x,Y = y) |
                    |--------------------------|

                (for continuous) >>
                    |--------------------|
                    | p(x) = ∫y p(x,y)dy |
                    |--------------------|
                    | p(y) = ∫x p(x,y)dx |
                    |--------------------|


->  Conditional density :- 
                    |-------------------------|
                    | p(X|Y) = p(X = x,Y = y) |
                    |          -------------- |
                    |               p(y)      |
                    |-------------------------|


->  Mean of X :
                    |---------------|
                    | E(X) = ∑xp(x) |
                    |---------------|
    Mean of Y :
                    |---------------|
                    | E(Y) = ∑yp(y) |
                    |---------------|
    Mean of XY :
                    |--------------------|
                    | E(XY) = ∑∑xyp(x,y) |
                    |--------------------|
    Mean of X+Y :
                    |------------------------|
                    | E(X+Y) = ∑∑(x+y)p(x,y) |
                    |------------------------|

















